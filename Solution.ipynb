{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x701f25fa2cf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "\n",
    "In the repository, it is already provided the linux version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains two agents playing tennis.  At each time step, it has two continuous actions at its disposal:\n",
    "- move left-right \n",
    "- jump\n",
    "\n",
    "The state space has `24` dimensions and contains the ball's position and velocity, along with the postion and velocity of the rackets.  A reward of `+0.1` is provided for pushing the ball over the net, and a reward of `-0.01` is provided for dropping the ball or pushing outside. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents count: 2\n",
      "State size: 24\n",
      "Action size: 2\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "NUM_AGENTS = len(env_info.agents)\n",
    "STATE_SIZE = env_info.vector_observations.shape[1]\n",
    "ACTION_SIZE = brain.vector_action_space_size\n",
    "\n",
    "print('Agents count:', NUM_AGENTS)\n",
    "print('State size:', STATE_SIZE)\n",
    "print('Action size:', ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it play tennis.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "Score: 0.0\n",
      "Score: 0.0\n",
      "Score: 0.0\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(NUM_AGENTS)                       # initialize the score\n",
    "    dones = np.zeros(NUM_AGENTS)                        # initialize the done\n",
    "    actions = np.zeros((NUM_AGENTS, ACTION_SIZE))\n",
    "    while not np.any(dones):\n",
    "        actions = np.random.randn(NUM_AGENTS, ACTION_SIZE)\n",
    "        # random actions but clipped to -1,1\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        # send the action to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        rewards = env_info.rewards                      # get the reward\n",
    "        dones = env_info.local_done                     # see if episode has finished\n",
    "        scores += rewards                               # update the score\n",
    "        next_states = env_info.vector_observations\n",
    "        states = next_states\n",
    "\n",
    "    max_score = np.max(scores)\n",
    "    print(\"Score: {}\".format(max_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "\n",
    "Familiarize with `agent.py`, `network.py`, `replay_buffer.py`.\n",
    "\n",
    "The `Agent` is the core of the algorithm, it predicts the next step. It store a local and a target network that are used to predict and train the the agent. It is an actor critic agent.\n",
    "\n",
    "The `ActorNetwork` and `CriticNetwork` are the deep networks we use for predicting the next move and the Q values for a given state.\n",
    "\n",
    "\n",
    "The `ReplayBuffer` is where we store the previous states/rewards/... we will sample this during the training.\n",
    "\n",
    "Most of the hyper parameter can be found in the `agent.py`.\n",
    "\n",
    "In the following block we are training the two agents. Note how we add some noise to the chosen actions in order to explore more the environment. We will gradually reduce the noise during the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent\n",
      "Actor: ActorNetwork(\n",
      "  (fc1): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Critic: CriticNetwork(\n",
      "  (fc1): Linear(in_features=26, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Agent\n",
      "Actor: ActorNetwork(\n",
      "  (fc1): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Critic: CriticNetwork(\n",
      "  (fc1): Linear(in_features=26, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Episode 0\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1500\n",
      "Episode 3\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/miniconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tScore: 0.0000\tMean Score: 0.0010\tMax Score: 0.1000\tStd: 0.1485\n",
      "Episode 200\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1470\n",
      "Episode 300\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1456\n",
      "Episode 400\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1441\n",
      "Episode 500\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1427\n",
      "Episode 600\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1413\n",
      "Episode 700\tScore: 0.0000\tMean Score: 0.0037\tMax Score: 0.1000\tStd: 0.1398\n",
      "Episode 800\tScore: 0.0000\tMean Score: 0.0054\tMax Score: 0.0900\tStd: 0.1385\n",
      "Episode 900\tScore: 0.0000\tMean Score: 0.0110\tMax Score: 0.1000\tStd: 0.1371\n",
      "Episode 1000\tScore: 0.0000\tMean Score: 0.0010\tMax Score: 0.1000\tStd: 0.1357\n",
      "Episode 1100\tScore: 0.0000\tMean Score: 0.0050\tMax Score: 0.1000\tStd: 0.1344\n",
      "Episode 1200\tScore: 0.0000\tMean Score: 0.0149\tMax Score: 0.1000\tStd: 0.1330\n",
      "Episode 1300\tScore: 0.0000\tMean Score: 0.0000\tMax Score: 0.0000\tStd: 0.1317\n",
      "Episode 1400\tScore: 0.0000\tMean Score: 0.0126\tMax Score: 0.0900\tStd: 0.1304\n",
      "Episode 1500\tScore: 0.0000\tMean Score: 0.0216\tMax Score: 0.0900\tStd: 0.1291\n",
      "Episode 1600\tScore: 0.1000\tMean Score: 0.0279\tMax Score: 0.1000\tStd: 0.1278\n",
      "Episode 1700\tScore: 0.0900\tMean Score: 0.0350\tMax Score: 0.2000\tStd: 0.1265\n",
      "Episode 1800\tScore: 0.0900\tMean Score: 0.0682\tMax Score: 0.2000\tStd: 0.1253\n",
      "Episode 1900\tScore: 0.0000\tMean Score: 0.0518\tMax Score: 0.2000\tStd: 0.1240\n",
      "Episode 2000\tScore: 0.0000\tMean Score: 0.0353\tMax Score: 0.2000\tStd: 0.1228\n",
      "Episode 2100\tScore: 0.0900\tMean Score: 0.0647\tMax Score: 0.3000\tStd: 0.1216\n",
      "Episode 2200\tScore: 0.0000\tMean Score: 0.0620\tMax Score: 0.2000\tStd: 0.1204\n",
      "Episode 2300\tScore: 0.1000\tMean Score: 0.0493\tMax Score: 0.1000\tStd: 0.1192\n",
      "Episode 2400\tScore: 0.1000\tMean Score: 0.0738\tMax Score: 0.2000\tStd: 0.1180\n",
      "Episode 2500\tScore: 0.0900\tMean Score: 0.0713\tMax Score: 0.2000\tStd: 0.1168\n",
      "Episode 2600\tScore: 0.2000\tMean Score: 0.1031\tMax Score: 0.3000\tStd: 0.1156\n",
      "Episode 2700\tScore: 0.0900\tMean Score: 0.1179\tMax Score: 0.3000\tStd: 0.1145\n",
      "Episode 2800\tScore: 0.1000\tMean Score: 0.1112\tMax Score: 0.3000\tStd: 0.1134\n",
      "Episode 2900\tScore: 0.2000\tMean Score: 0.1166\tMax Score: 0.2900\tStd: 0.1122\n",
      "Episode 3000\tScore: 0.2000\tMean Score: 0.1161\tMax Score: 0.4000\tStd: 0.1111\n",
      "Episode 3100\tScore: 0.1900\tMean Score: 0.1324\tMax Score: 0.4000\tStd: 0.1100\n",
      "Episode 3200\tScore: 0.4000\tMean Score: 0.1912\tMax Score: 0.6000\tStd: 0.1089\n",
      "Episode 3300\tScore: 0.4000\tMean Score: 0.3189\tMax Score: 1.4000\tStd: 0.1078\n",
      "Episode 3400\tScore: 0.7000\tMean Score: 0.4985\tMax Score: 2.6000\tStd: 0.1068\n",
      "Episode 3403\tScore: 0.6000\tMean Score: 0.5017\tMax Score: 2.6000\tStd: 0.1067\n",
      "Solved in 3403 episodes.\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch.distributions import normal\n",
    "\n",
    "agent_zero = Agent(STATE_SIZE, ACTION_SIZE)\n",
    "agent_one = Agent(STATE_SIZE, ACTION_SIZE)\n",
    "print(agent_zero)\n",
    "print(agent_one)\n",
    "\n",
    "std_initial = 0.15\n",
    "std_final = 0.05\n",
    "std_decrease = 1.0-1.0e-4\n",
    "std = std_initial\n",
    "\n",
    "target = 0.5\n",
    "scores=[]\n",
    "scores_window = deque(maxlen=100)\n",
    "for i_episode in range(10000):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    score = np.zeros(NUM_AGENTS)\n",
    "\n",
    "    noise_distribution = normal.Normal(0, std)\n",
    "    while True:\n",
    "        action_zero = agent_zero.act(\n",
    "            state[0]) + noise_distribution.sample((ACTION_SIZE,))\n",
    "        action_one = agent_one.act(\n",
    "            state[1]) + noise_distribution.sample((ACTION_SIZE,))\n",
    "\n",
    "        actions = np.vstack((action_zero.numpy(), action_one.numpy()))\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        agent_zero.step(state[0], actions[0], rewards[0],\n",
    "                        next_state[0], dones[0])\n",
    "        agent_one.step(state[1], actions[1], rewards[1],\n",
    "                       next_state[1], dones[1])\n",
    "        score += env_info.rewards\n",
    "        state = next_state\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    std = max(std*std_decrease, std_final)\n",
    "    score = np.max(score)\n",
    "    \n",
    "    scores.append(score)\n",
    "    scores_window.append(score)\n",
    "    average_score = np.mean(scores_window)\n",
    "    max_score = np.max(scores_window)\n",
    "    \n",
    "    print('\\rEpisode {}\\tScore: {:.4f}\\tMean Score: {:.4f}\\tMax Score: {:.4f}\\tStd: {:.4f}'.format(\n",
    "        i_episode, score, average_score, max_score, std), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tScore: {:.4f}\\tMean Score: {:.4f}\\tMax Score: {:.4f}\\tStd: {:.4f}'.format(\n",
    "            i_episode, score, average_score, max_score, std))\n",
    "    if average_score >= target:\n",
    "        print(\"\\nSolved in {} episodes.\".format(i_episode))\n",
    "        agent_zero.save(\"agent_zero.pth\")\n",
    "        agent_one.save(\"agent_one.pth\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episode #')\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our trained agent without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: [2.50000004 2.60000004]\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "trained_agent_zero = Agent(STATE_SIZE, ACTION_SIZE)\n",
    "trained_agent_one = Agent(STATE_SIZE, ACTION_SIZE)\n",
    "trained_agent_zero.load('agent_zero.pth')\n",
    "trained_agent_one.load('agent_one.pth')\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations \n",
    "score = np.zeros(NUM_AGENTS)                       # initialize the score\n",
    "while True:\n",
    "    action_zero = trained_agent_zero.act(state[0])\n",
    "    action_one = trained_agent_one.act(state[1])\n",
    "    action = np.vstack((action_zero.numpy(), action_one.numpy()))\n",
    "    env_info = env.step(action)[brain_name]     # send the action to the environment\n",
    "    next_state = env_info.vector_observations   # get the next state\n",
    "    reward = env_info.rewards                   # get the reward\n",
    "    done = env_info.local_done                  # see if episode has finished\n",
    "    score += reward                             # update the score\n",
    "    state = next_state                          # roll over the state to next time step\n",
    "    if np.any(done):                           # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
